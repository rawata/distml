{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "colab": {
      "name": "006.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGGVFaU0e_bd"
      },
      "source": [
        "## Unsupervised Learning  - Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FrMjT8spe_be"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "Building a dataset of documents\n",
        "'''\n",
        "\n",
        "'''\n",
        "    Training docs\n",
        "'''\n",
        "doc0 = \"hello hello world \"\n",
        "doc1 = \"hello world world\"\n",
        "doc2 = \"hello world hello world\"\n",
        "doc3 = \"foo foo bar \"\n",
        "doc4 = \"foo bar bar\"\n",
        "doc5 = \"foo bar foo bar\"\n",
        "doc6 = \"lottery prize winner\"\n",
        "doc7 = \"lottery prize\"\n",
        "doc8 = \"lottery lottery lottery prize winner\"\n",
        "docs_train = [doc0, doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8]\n",
        "\n",
        "'''\n",
        "    Test docs\n",
        "'''\n",
        "doct0 = \"foo bar\"\n",
        "doct1 = \"lottery prize hello\"\n",
        "docs_test = [doct0, doct1]\n",
        "\n",
        "\n",
        "'''\n",
        "    Converting documents to feature vectors\n",
        "'''\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(docs_train).toarray()\n",
        "features = vectorizer.get_feature_names()\n",
        "print(f'Features: {features}')\n",
        "print(f'X_train:\\n====\\n{X_train}\\n====\\n')\n",
        "X_test = vectorizer.transform(docs_test).toarray()\n",
        "print(f'X_test:\\n====\\n{X_test}\\n====\\n')\n",
        "\n",
        "'''\n",
        "Choosing the model\n",
        "'''\n",
        "model = NearestNeighbors(n_neighbors=3, algorithm='brute')\n",
        "\n",
        "'''\n",
        "    Train the model\n",
        "'''\n",
        "#TBD Train the model\n",
        "\n",
        "'''\n",
        "    Test the model, by getting  k=3 most similar documents, and their distances to the ones in test docs \n",
        "'''\n",
        "#TBD Test the model\n",
        "\n",
        "indices, distances = (0,0)\n",
        "\n",
        "print(f'Indices of nearest documents:\\n====\\n{indices}\\n====\\n')\n",
        "print(f'Distances of nearest documents:\\n====\\n{distances}\\n====\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMxWHXRke_bj"
      },
      "source": [
        "## Unsupervised Learning  - Anomaly detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2PTSoPJe_bj"
      },
      "source": [
        "import random\n",
        "\n",
        "'''\n",
        "features = [\"amount_category\", \"merchant_web_mobile\", \"dow\",  \"hour\" ]\n",
        "feature description\n",
        "    amount_category       1 if amount < £100\n",
        "                          2 if  £100 <= amount < £500)\n",
        "                          3 if  £500 <= amount\n",
        "\n",
        "    merchant_web_mobile   1 if  transaction done in shop\n",
        "                          2 if  transaction done through web\n",
        "                          3 if  transaction done through mobile    \n",
        "    \n",
        "    dow                   transaction day of week (1-7)\n",
        "    hour                  transaction hour (1-24)\n",
        "'''\n",
        "def normal_txn_hour():\n",
        "    return random.randint(9,18)\n",
        "def normal_txn_dow():\n",
        "    return random.randint(1,5)\n",
        "\n",
        "'''\n",
        "Building a hypothetical dataset of normal transactions\n",
        "'''\n",
        "N = 1000\n",
        "X_train = [[1, 1, normal_txn_dow(), normal_txn_hour()] for i in range(N)]\n",
        "# print(*X_train, sep = \"\\n\")\n",
        "\n",
        "'''\n",
        "Building a hypothetical test dataset: some fraud transactions and some normal\n",
        "'''\n",
        "X_test = [\n",
        "    [2,2,6,23],\n",
        "    [3,2,7,23],\n",
        "    [1,0,2,11],\n",
        "]\n",
        "\n",
        "'''\n",
        "Choosing a model\n",
        "'''\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "model = EllipticEnvelope(contamination=0.0)\n",
        "\n",
        "'''\n",
        "    Train the model so that it understand whats normal transaction\n",
        "'''\n",
        "\n",
        "#TBD\n",
        "\n",
        "\n",
        "'''\n",
        "   Use the trained model to predict if the given transactions are fraudlent or not\n",
        "'''\n",
        "\n",
        "#TBD \n",
        "predictions = []\n",
        "\n",
        "print(f'Fraud Transaction Prediction (-1 means anomaly/outlier i.e fraud):\\n====\\n{predictions}\\n====\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU_05K1ge_bm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}